{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#We use function spect_loader extracted from \n",
    "#(https://github.com/adiyoss/GCommandsPytorch/blob/master/gcommand_loader.py)\n",
    "#to calculate the spectrogram of the audio file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brooks\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train freq\n",
      "   down  1906\n",
      "     go  1917\n",
      "   left  1900\n",
      "     no  1914\n",
      "    off  1883\n",
      "     on  1906\n",
      "  right  1894\n",
      "silence  341\n",
      "   stop  1919\n",
      "unknown  2196\n",
      "     up  1903\n",
      "    yes  1906\n",
      "\n",
      "val freq\n",
      "   down  453\n",
      "     go  455\n",
      "   left  453\n",
      "     no  461\n",
      "    off  474\n",
      "     on  461\n",
      "  right  473\n",
      "silence  57\n",
      "   stop  461\n",
      "unknown  670\n",
      "     up  472\n",
      "    yes  471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import Iterator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "import librosa\n",
    "import os\n",
    "import multiprocessing.pool\n",
    "from functools import partial\n",
    "from random import getrandbits\n",
    "\n",
    "train_path='C:/Users/Brooks/Desktop/part1/train'\n",
    "val_path ='C:/Users/Brooks/Desktop/part1/val'\n",
    "\n",
    "\n",
    "#to count how many files inside each class\n",
    "classnames=os.listdir(train_path)\n",
    "train_count_dict = {}\n",
    "for d in classnames:\n",
    "    train_count_dict[d] = len(os.listdir(os.path.join(train_path, d)))\n",
    "print('train freq')\n",
    "for k, v in train_count_dict.items():\n",
    "    print ( '%7s  %i' % (k, v))\n",
    "val_count_dict = {}\n",
    "for d in classnames:\n",
    "    val_count_dict[d] = len(os.listdir(os.path.join(val_path, d)))\n",
    "print('\\nval freq')\n",
    "for k, v in val_count_dict.items():\n",
    "    print ( '%7s  %i' % (k, v))\n",
    "print ('')\n",
    "#print ('test files', len(os.listdir(test_path+'/audio')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Convert the sound to melspectrogram#\n",
    "#We use function spect_loader extracted from \n",
    "#(https://github.com/adiyoss/GCommandsPytorch/blob/master/gcommand_loader.py)\n",
    "#to calculate the spectrogram of the audio file\n",
    "\n",
    "#However we decide to use melspectrogram instead of spectrogram since some research \n",
    "#found that while using MFCC, the performance will better;\n",
    "#So we use a similar chart to MFCC which is mel-spectrogram\n",
    "def spect_loader(path, window_size, window_stride, window, normalize, max_len=101, \n",
    "                 augment=False, allow_speedandpitch=False, allow_pitch=False,\n",
    "                 allow_speed=False, allow_dyn=False, allow_noise=False,\n",
    "                allow_timeshift=False ):\n",
    "    y, sr = librosa.load(path, sr=None)\n",
    "    #OriginalSampelRate=16Khz;\n",
    "    # n_fft = 4096\n",
    "    n_fft = int(sr * window_size)\n",
    "    win_length = n_fft\n",
    "    hop_length = int(sr * window_stride)\n",
    "    \n",
    "    # Let's make and display a mel-scaled power (energy-squared) spectrogram\n",
    "    S = librosa.feature.melspectrogram(y, sr=sr,  n_fft = 4096,hop_length=hop_length,n_mels=128)\n",
    "\n",
    "    # Convert to log scale (dB). We'll use the peak power as reference.\n",
    "    log_S = librosa.core.amplitude_to_db(S, ref=np.max)\n",
    "\n",
    "    \n",
    "    #made a melspectrogram\n",
    "    spect=log_S\n",
    "    \n",
    "    # make all spects with the same dimentions\n",
    "    #In case of the mel-spectrogram size different since the lenght of the sound variate\n",
    "    # TODO: change that in the future\n",
    "    if spect.shape[1] < max_len:\n",
    "        pad = np.zeros((spect.shape[0], max_len - spect.shape[1]))\n",
    "        spect = np.hstack((spect, pad))\n",
    "    elif spect.shape[1] > max_len:\n",
    "        spect = spect[:max_len, ]\n",
    "    spect = np.resize(spect, (1, spect.shape[0], spect.shape[1]))\n",
    "    #spect = torch.FloatTensor(spect)\n",
    "    \n",
    "    \n",
    "    # z-score normalization\n",
    "    #calculate the mean and standard of the melspect\n",
    "    if normalize:\n",
    "        mean = np.mean(np.ravel(spect))\n",
    "        std = np.std(np.ravel(spect))\n",
    "        if std != 0:\n",
    "            spect = spect -mean\n",
    "            spect = spect / std\n",
    "    return spect\n",
    "\n",
    "def _count_valid_files_in_directory(directory, white_list_formats, follow_links):\n",
    "    \"\"\"Count files with extension in `white_list_formats` contained in a directory.\n",
    "    # Arguments\n",
    "        directory: absolute path to the directory containing files to be counted\n",
    "        white_list_formats: set of strings containing allowed extensions for\n",
    "            the files to be counted.\n",
    "    # Returns\n",
    "        the count of files with extension in `white_list_formats` contained in\n",
    "        the directory.\n",
    "    \"\"\"\n",
    "    def _recursive_list(subpath):\n",
    "        return sorted(os.walk(subpath, followlinks=follow_links), key=lambda tpl: tpl[0])\n",
    "\n",
    "    samples = 0\n",
    "    for root, _, files in _recursive_list(directory):\n",
    "        for fname in files:\n",
    "            is_valid = False\n",
    "            for extension in white_list_formats:\n",
    "                if fname.lower().endswith('.' + extension):\n",
    "                    is_valid = True\n",
    "                    break\n",
    "            if is_valid:\n",
    "                samples += 1\n",
    "    return samples\n",
    "\n",
    "def _list_valid_filenames_in_directory(directory, white_list_formats,\n",
    "                                       class_indices, follow_links):\n",
    "    \"\"\"List paths of files in `subdir` relative from `directory` whose extensions are in `white_list_formats`.\n",
    "    # Arguments\n",
    "        directory: absolute path to a directory containing the files to list.\n",
    "            The directory name is used as class label and must be a key of `class_indices`.\n",
    "        white_list_formats: set of strings containing allowed extensions for\n",
    "            the files to be counted.\n",
    "        class_indices: dictionary mapping a class name to its index.\n",
    "    # Returns\n",
    "        classes: a list of class indices\n",
    "        filenames: the path of valid files in `directory`, relative from\n",
    "            `directory`'s parent (e.g., if `directory` is \"dataset/class1\",\n",
    "            the filenames will be [\"class1/file1.jpg\", \"class1/file2.jpg\", ...]).\n",
    "    \"\"\"\n",
    "    def _recursive_list(subpath):\n",
    "        return sorted(os.walk(subpath, followlinks=follow_links), key=lambda tpl: tpl[0])\n",
    "\n",
    "    classes = []\n",
    "    filenames = []\n",
    "    subdir = os.path.basename(directory)\n",
    "    basedir = os.path.dirname(directory)\n",
    "    for root, _, files in _recursive_list(directory):\n",
    "        for fname in sorted(files):\n",
    "            is_valid = False\n",
    "            for extension in white_list_formats:\n",
    "                if fname.lower().endswith('.' + extension):\n",
    "                    is_valid = True\n",
    "                    break\n",
    "            if is_valid:\n",
    "                classes.append(class_indices[subdir])\n",
    "                # add filename relative to directory\n",
    "                absolute_path = os.path.join(root, fname)\n",
    "                filenames.append(os.path.relpath(absolute_path, basedir))\n",
    "    return classes, filenames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #window_size=.02\n",
    "    #window_stride=.01\n",
    "    #window_type='hamming'\n",
    "    #normalize=True\n",
    "    #max_len=101\n",
    "    #batch_size = 64\n",
    "class SpeechDirectoryIterator(Iterator):\n",
    "    \"\"\"Iterator capable of reading images from a directory on disk.\n",
    "    # Arguments\n",
    "       \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, directory, window_size, window_stride, \n",
    "                 window_type, normalize, max_len=101,\n",
    "                 target_size=(256, 256), color_mode='grayscale',\n",
    "                 classes=None, class_mode='categorical',\n",
    "                 batch_size=32, shuffle=True, seed=None,\n",
    "                 data_format=None, save_to_dir=None,\n",
    "                 save_prefix='', save_format='png',\n",
    "                 follow_links=False, interpolation='nearest', augment=False,\n",
    "                allow_speedandpitch = False, allow_pitch = False,\n",
    "                allow_speed = False, allow_dyn = False, allow_noise = False, allow_timeshift=False ):\n",
    "        if data_format is None:\n",
    "            data_format = K.image_data_format()\n",
    "        self.window_size = window_size\n",
    "        self.window_stride = window_stride\n",
    "        self.window_type = window_type\n",
    "        self.normalize = normalize\n",
    "        self.max_len = max_len\n",
    "        self.directory = directory\n",
    "        self.allow_speedandpitch = allow_speedandpitch\n",
    "        self.allow_pitch = allow_pitch\n",
    "        self.allow_speed = allow_speed \n",
    "        self.allow_dyn = allow_dyn\n",
    "        self.allow_noise = allow_noise\n",
    "        self.allow_timeshift = allow_timeshift \n",
    "        self.augment = augment\n",
    "#        self.image_data_generator = image_data_generator\n",
    "        self.target_size = tuple(target_size)\n",
    "        if color_mode not in {'rgb', 'grayscale'}:\n",
    "            raise ValueError('Invalid color mode:', color_mode,\n",
    "                             '; expected \"rgb\" or \"grayscale\".')\n",
    "        self.color_mode = color_mode\n",
    "        self.data_format = data_format\n",
    "        if self.color_mode == 'rgb':\n",
    "            #becasue it is rgb, so the hight of the data is 3\n",
    "            if self.data_format == 'channels_last':\n",
    "                self.image_shape = self.target_size + (3,)\n",
    "            else:\n",
    "                self.image_shape = (3,) + self.target_size\n",
    "        else:\n",
    "            #if it's gray scale, the picture's hight is 1\n",
    "            if self.data_format == 'channels_last':\n",
    "                self.image_shape = self.target_size + (1,)\n",
    "            else:\n",
    "                self.image_shape = (1,) + self.target_size\n",
    "        self.classes = classes\n",
    "        if class_mode not in {'categorical', 'binary', 'sparse',\n",
    "                              'input', None}:\n",
    "            raise ValueError('Invalid class_mode:', class_mode,\n",
    "                             '; expected one of \"categorical\", '\n",
    "                             '\"binary\", \"sparse\", \"input\"'\n",
    "                             ' or None.')\n",
    "        self.class_mode = class_mode\n",
    "        self.save_to_dir = save_to_dir\n",
    "        self.save_prefix = save_prefix\n",
    "        self.save_format = save_format\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "        white_list_formats = {'png', 'jpg', 'jpeg', 'bmp', 'ppm', 'wav'}\n",
    "\n",
    "        # first, count the number of samples and classes\n",
    "        self.samples = 0\n",
    "\n",
    "        if not classes:\n",
    "            classes = []\n",
    "            for subdir in sorted(os.listdir(directory)):\n",
    "                if os.path.isdir(os.path.join(directory, subdir)):\n",
    "                    classes.append(subdir)\n",
    "        self.num_classes = len(classes)\n",
    "        self.class_indices = dict(zip(classes, range(len(classes))))\n",
    "\n",
    "        pool = multiprocessing.pool.ThreadPool()\n",
    "        function_partial = partial(_count_valid_files_in_directory,\n",
    "                                   white_list_formats=white_list_formats,\n",
    "                                   follow_links=follow_links)\n",
    "        self.samples = sum(pool.map(function_partial,\n",
    "                                    (os.path.join(directory, subdir)\n",
    "                                     for subdir in classes)))\n",
    "\n",
    "        print('Found %d images belonging to %d classes.' % (self.samples, self.num_classes))\n",
    "\n",
    "        # second, build an index of the images in the different class subfolders\n",
    "        results = []\n",
    "\n",
    "        self.filenames = []\n",
    "        self.classes = np.zeros((self.samples,), dtype='int32')\n",
    "        i = 0\n",
    "        for dirpath in (os.path.join(directory, subdir) for subdir in classes):\n",
    "            results.append(pool.apply_async(_list_valid_filenames_in_directory,\n",
    "                                            (dirpath, white_list_formats,\n",
    "                                             self.class_indices, follow_links)))\n",
    "            \n",
    "        \n",
    "        for res in results:\n",
    "            classes, filenames = res.get()\n",
    "            self.classes[i:i + len(classes)] = classes\n",
    "            self.filenames += filenames\n",
    "            if i==0:\n",
    "                #Use the specloader to trans the .wav file to image\n",
    "                img = spect_loader(os.path.join(self.directory, filenames[0]), \n",
    "                               self.window_size, \n",
    "                               self.window_stride, \n",
    "                               self.window_type, \n",
    "                               self.normalize, \n",
    "                               self.max_len, \n",
    "                               self.augment,\n",
    "                               self.allow_speedandpitch,\n",
    "                               self.allow_pitch,\n",
    "                               self.allow_speed, \n",
    "                               self.allow_dyn,\n",
    "                               self.allow_noise,\n",
    "                               self.allow_timeshift ) \n",
    "                img=np.swapaxes(img, 0, 2)\n",
    "                self.target_size = tuple((img.shape[0], img.shape[1]))\n",
    "                print(self.target_size)\n",
    "                if self.color_mode == 'rgb':\n",
    "                    if self.data_format == 'channels_last':\n",
    "                        self.image_shape = self.target_size + (3,)\n",
    "                    else:\n",
    "                        self.image_shape = (3,) + self.target_size\n",
    "                else:\n",
    "                    if self.data_format == 'channels_last':\n",
    "                        self.image_shape = self.target_size + (1,)\n",
    "                    else:\n",
    "                        self.image_shape = (1,) + self.target_size\n",
    "                        \n",
    "            i += len(classes)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        super(SpeechDirectoryIterator, self).__init__(self.samples, batch_size, shuffle, seed)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=K.floatx())\n",
    "        batch_f = []\n",
    "        grayscale = self.color_mode == 'grayscale'\n",
    "        # build batch of image data\n",
    "        #print(index_array)\n",
    "        for i, j in enumerate(index_array):\n",
    "            #print(i, j, self.filenames[j])\n",
    "            fname = self.filenames[j]\n",
    "            #img = load_img(os.path.join(self.directory, fname),\n",
    "            #               grayscale=grayscale,\n",
    "            #               target_size=self.target_size,\n",
    "            #               interpolation=self.interpolation)\n",
    "            img = spect_loader(os.path.join(self.directory, fname), \n",
    "                               self.window_size, \n",
    "                               self.window_stride, \n",
    "                               self.window_type, \n",
    "                               self.normalize, \n",
    "                               self.max_len, \n",
    "                                )\n",
    "            img=np.swapaxes(img, 0, 2)\n",
    "            \n",
    "            x = img_to_array(img, data_format=self.data_format)\n",
    "            #x = self.image_data_generator.random_transform(x)\n",
    "            #x = self.image_data_generator.standardize(x)\n",
    "            batch_x[i] = x\n",
    "            batch_f.append(fname)\n",
    "        # optionally save augmented images to disk for debugging purposes\n",
    "        if self.save_to_dir:\n",
    "            for i, j in enumerate(index_array):\n",
    "                img = array_to_img(batch_x[i], self.data_format, scale=True)\n",
    "                fname = '{prefix}_{index}_{hash}.{format}'.format(prefix=self.save_prefix,\n",
    "                                                                  index=j,\n",
    "                                                                  hash=np.random.randint(1e7),\n",
    "                                                                  format=self.save_format)\n",
    "                img.save(os.path.join(self.save_to_dir, fname))\n",
    "        # build batch of labels\n",
    "        if self.class_mode == 'input':\n",
    "            batch_y = batch_x.copy()\n",
    "        elif self.class_mode == 'sparse':\n",
    "            batch_y = self.classes[index_array]\n",
    "        elif self.class_mode == 'binary':\n",
    "            batch_y = self.classes[index_array].astype(K.floatx())\n",
    "        elif self.class_mode == 'categorical':\n",
    "            batch_y = np.zeros((len(batch_x), self.num_classes), dtype=K.floatx())\n",
    "            for i, label in enumerate(self.classes[index_array]):\n",
    "                batch_y[i, label] = 1.\n",
    "        else:\n",
    "            return batch_x\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def next(self):\n",
    "        with self.lock:\n",
    "            index_array = next(self.index_generator)[0]\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        return self._get_batches_of_transformed_samples(index_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#window_size: this quantity times the sampling rate (sr) returns the fft window size\n",
    "#window_stride: Each frame of audio is windowed by window_stride * sr\n",
    "#window_type: The type of window function to be applied (hamming, hanning, etc)\n",
    "#normalize: True / Fale\n",
    "#max_len: Keep only max_len frequency components (first dimension of the output numpy array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21585 images belonging to 12 classes.\n",
      "(101, 128)\n"
     ]
    }
   ],
   "source": [
    "window_size=.02\n",
    "window_stride=.01\n",
    "window_type='hamming'\n",
    "normalize=True\n",
    "max_len=101\n",
    "batch_size = 64\n",
    "#put the train file to train_iterator\n",
    "train_iterator = SpeechDirectoryIterator(directory=train_path, \n",
    "                                   batch_size=batch_size, \n",
    "                                   window_size=window_size, \n",
    "                                   window_stride=window_stride, \n",
    "                                   window_type=window_type,\n",
    "                                   normalize=normalize, \n",
    "                                   max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5361 images belonging to 12 classes.\n",
      "(101, 128)\n"
     ]
    }
   ],
   "source": [
    "#put the validation file to val_iterator\n",
    "val_iterator = SpeechDirectoryIterator(directory=val_path, \n",
    "                                   batch_size=batch_size, \n",
    "                                   window_size=window_size, \n",
    "                                   window_stride=window_stride, \n",
    "                                   window_type=window_type,\n",
    "                                   normalize=normalize, \n",
    "                                   max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 99, 126, 32)       320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 97, 124, 64)       18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 48, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 190464)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               24379520  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                1548      \n",
      "=================================================================\n",
      "Total params: 24,399,884\n",
      "Trainable params: 24,399,884\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "#build the training model by keras\n",
    "\n",
    "#Use model structure similar to the MNIST recognition\n",
    "#And we have made some optimize of the parameters include the costfunc and optimizer ect.\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=train_iterator.image_shape))\n",
    "# add a 2d Convolution with 3*3 kernel and 64 output.\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#Add a maxpooling with 2*2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#with 0.25 margin to Dropout \n",
    "model.add(Dropout(0.5))\n",
    "#flat all the pixel\n",
    "model.add(Flatten())\n",
    "#use fully connected and the activation is relu\n",
    "model.add(Dense(128, activation='relu'))\n",
    "#with 0.5 margin drop out\n",
    "model.add(Dropout(0.5))\n",
    "#use the softmax as the activation function\n",
    "model.add(Dense(len(classnames), activation = 'softmax')) #Last layer with one output per class\n",
    "\n",
    "#Use crossEntropy as the loss function \n",
    "#And set the optimizer as Adadelta\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "338/338 [==============================] - 508s 2s/step - loss: 1.9102 - acc: 0.3422 - val_loss: 1.3871 - val_acc: 0.5544\n",
      "Epoch 2/20\n",
      "338/338 [==============================] - 424s 1s/step - loss: 1.3435 - acc: 0.5346 - val_loss: 1.0789 - val_acc: 0.6726\n",
      "Epoch 3/20\n",
      "338/338 [==============================] - 380s 1s/step - loss: 1.0506 - acc: 0.6359 - val_loss: 0.9396 - val_acc: 0.7140\n",
      "Epoch 4/20\n",
      "338/338 [==============================] - 381s 1s/step - loss: 0.8727 - acc: 0.6962 - val_loss: 0.9129 - val_acc: 0.7168\n",
      "Epoch 5/20\n",
      "338/338 [==============================] - 381s 1s/step - loss: 0.7498 - acc: 0.7426 - val_loss: 0.8929 - val_acc: 0.7439\n",
      "Epoch 6/20\n",
      "338/338 [==============================] - 381s 1s/step - loss: 0.6511 - acc: 0.7722 - val_loss: 0.9192 - val_acc: 0.7379\n",
      "Epoch 7/20\n",
      "338/338 [==============================] - 381s 1s/step - loss: 0.5725 - acc: 0.7993 - val_loss: 0.9771 - val_acc: 0.7431\n",
      "Epoch 8/20\n",
      "338/338 [==============================] - 382s 1s/step - loss: 0.5253 - acc: 0.8130 - val_loss: 0.9288 - val_acc: 0.7532\n",
      "Epoch 9/20\n",
      "338/338 [==============================] - 382s 1s/step - loss: 0.4769 - acc: 0.8295 - val_loss: 0.9974 - val_acc: 0.7424\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.5.\n",
      "Epoch 10/20\n",
      "338/338 [==============================] - 379s 1s/step - loss: 0.3763 - acc: 0.8669 - val_loss: 0.9612 - val_acc: 0.7573\n",
      "Epoch 00010: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21a6bac1048>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "#Use this func to early stop when the val_loss do not lower\n",
    "#Use this ReduceLROnPlateau to auto adapt the LearningRate\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, mode='auto', min_lr=0.00001)\n",
    "model.fit_generator(train_iterator,\n",
    "        steps_per_epoch=int(np.ceil(train_iterator.n / batch_size)),\n",
    "        epochs=20,\n",
    "        validation_data=val_iterator,\n",
    "        validation_steps=int(np.ceil(val_iterator.n / batch_size)),\n",
    "        verbose=1, callbacks=[early, reduce])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will we do in the future:\n",
    "draw the acc-loss chart to show the progress of the training;\n",
    "continue to optimize the parameter of the network;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
